Première url trouvée : /robots.txt
Url : /.hidden

Comment se protéger : interdire l’accès ou rediriger les utilisateurs s'ils tentent d'accéder à des fichiers que nous ne voulons pas indexer.
Comment on a trouvé : via la page /robots.txt qui sert à indiquer aux robots des moteurs de recherches les urls qu'il peut scanner sur notre site. C'est pour alléger le nomber de requêtes reçues et protéger le serveur d'une surcharge.


Il fallait trouver le flag parmi tout un tas de dossiers et sous-dossiers et fichiers. Un script bash nous a permi de trouver :
FLAG trouvé dans http://192.168.1.70/.hidden/whtccjokayshttvxycsvykxcfm/igeemtxnvexvxezqwntmzjltkt/lmpanswobhwcozdqixbowvbrhw/README

#!/bin/bash

baseUrl="http://192.168.1.70/.hidden/"

chercher_flag(){
    local url="$1"

    echo "Visite de : $url"

    contenu=$(curl -s "$url")

    liens=$(echo "$contenu" | grep -o 'href="[A-Za-z]*' | sed 's/href="//')

    for lien in $liens; do
        if [ "$lien" == "README" ]; then
            readme_content=$(curl -s "$url$lien")


            if echo "$readme_content" | grep -q "flag"; then
                echo -e "\nFLAG trouvé dans $url$lien :\n"
                echo "$readme_content"
                exit 0
            fi
        elif [ "$lien" != "" ]; then

            chercher_flag "$url$lien/"
        fi
    done
}

chercher_flag "$baseUrl"

echo -e "\nAucun FLAG trouvé."
